<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author"      content="Sergey Pozhilov (GetTemplate.com)">
	
	<title>Alina Roitberg - Personal Webpage</title>

	<link rel="shortcut icon" href="assets/images/gt_favicon.png">
	
	<!-- Bootstrap -->
	<link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
	<!-- Icons -->
	<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
	<!-- Fonts -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
	<!-- Custom styles -->
	<link rel="stylesheet" href="assets/css/styles.css">
	 <style>
	 
        /* Style for the horizontal bar */
        hr {
            border: none; /* Remove default border */
            height: 1px; /* Set the height of the bar */
            background-color: #bbc5c2; /* Set the background color of the bar */
            margin: 0px 0; /* Add some spacing above and below the bar */
			 width: 100%;
        }
		ul { 
  list-style: none; 
  padding-left: 0; 
  margin-left: 0; 
}

ul li { 
  list-style: none; 
  margin: 30px 0; 
}
    </style>
	
	

	<!--[if lt IE 9]> <script src="assets/js/html5shiv.js"></script> <![endif]-->
</head>
<body class="home">

<header id="header">
	<div id="head" class="parallax" parallax-speed="2">
		<h1 id="logo" class="text-center">
			<img class="img-circle" src="assets/images/alina.jpeg" alt="">
			<span class="title">Alina Roitberg</span>
			<span class="tagline">Full Professor of Machine Learning at the University of Hildesheim<br> 
				<a href="">roitberg [at] uni-hildesheim.de</a></span>
		</h1>
	</div>
	<hr>
<nav class="navbar navbar-default navbar-sticky">
    <div class="container-fluid">

        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"> 
                <span class="sr-only">Toggle navigation</span> 
                <span class="icon-bar"></span> 
                <span class="icon-bar"></span> 
                <span class="icon-bar"></span> 
            </button>
        </div>

        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav">
               <li><a href="index.html"><b>Home</b></a></li>
                <li class="active"><a href="publications.html"><b>Publications</b></a></li>
                <!--<li><a href="teaching.html"><b>Teaching</b></a></li>-->
                <!-- Deactivated per request: Theses/Jobs -->
                <!-- <li><a href="teaching.html"><b>Theses</b></a></li> -->
                <!--<li><a href="group.html"><b>Group</b></a></li>-->
                <!-- <li><a href="applications.html"><b>Applications / Jobs</b></a></li> -->
            </ul>
        </div>

    </div>
</nav>
</header>

<main id="main">

	<div class="container">
		
		
		
	
		

		<div class="row section clients topspace">
			<h2 class="section-title"><span>Publications</span></h2>
			<div class="col-lg-12">
				<p> For a complete and up-to-date publication list, check my <a href="https://scholar.google.com/citations?user=UuEFRDoAAAAJ&hl=en&oi=ao">Google Scholar profile</a>.<p>
				
				
<ul>


<ul>

<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2026</div> -->


<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Order Matters: On Parameter-Efficient Image-to-Video Probing for Recognizing Nearly Symmetric Actions.</strong><br>
  T. Thiyakesan Ponbagavathi, A. Roitberg.<br>
  <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2026 (accepted).
  <strong>[<a href="https://arxiv.org/pdf/2503.24298.pdf">pdf</a>]</strong>
  <span style="color:#666;"><strong>[code coming soon]</strong></span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Mitigating Label Noise using Prompt-Based Hyperbolic Meta-Learning in Open-Set Domain Generalization.</strong><br>
  K. Peng, D. Wen, M. Saquib Sarfraz, Y. Chen, J. Zheng, D. Schneider, K. Yang, J. Wu, A. Roitberg, R. Stiefelhagen.<br>
  <em>International Journal of Computer Vision (IJCV)</em>, 2026.
  <strong>[<a href="https://arxiv.org/pdf/2412.18342.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/HyProMeta">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>When Surgery Meets the Unknown: Uncertainty-Aware Open-Set Recognition for Surgery Phase Classification.</strong><br>
  S. Geyer, V. Kalogeiton, A. Roitberg.<br>
  <em>International Conference on Computer Vision Theory and Applications (VISAPP)</em>, 2026 (accepted).
  <span style="color:#666;"><strong>[pdf &amp; code coming soon]</strong></span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>PDNet: Exploring Pruning and Distillation for Real-Time Worker Activity Recognition in Industrial Assembly.</strong><br>
  M. Baba, T. Thiyakesan Ponbagavathi, A. Roitberg.<br>
  <em>Conference paper</em>, 2026. <span style="color:#666;">(based on a master‚Äôs thesis)</span>
  <span style="color:#666;"><strong>[pdf &amp; code coming soon]</strong></span>
</li>



<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2025</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>T-MASK: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring.</strong><br>
  T. Thiyakesan Ponbagavathi, K. Peng, A. Roitberg.<br>
  <em>IEEE Intelligent Transportation Systems Conference (ITSC)</em>, Naples, Italy, 2025.
  <strong>üèÜ Best Student Paper Award Finalist (Top 5)</strong>.
  <strong>[<a href="https://arxiv.org/pdf/2508.16207.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/th-nesh/T-MASK">code coming soon</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Deep Learning for Metabolic Rate Estimation from Biosignals: A Comparative Study of Architectures and Signal Selection.</strong><br>
  S. Babakhani, C. David Remy, A. Roitberg.<br>
  <em>BMVC 2025 Workshop on Multisensory Intelligence for Human Perception</em>, 2025.
  <strong>[<a href="https://bmva-archive.org.uk/bmvc/2025/assets/workshops/MPI/Paper_1/paper.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/Sarvibabakhani/deeplearning-biosignals-ee">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Exploring Self-supervised Skeleton-based Action Recognition in Occluded Environments.</strong><br>
  Y. Chen, K. Peng, A. Roitberg, D. Schneider, J. Zhang, J. Zheng, Y. Chen, R. Liu, K. Yang, R. Stiefelhagen.<br>
  <em>International Joint Conference on Neural Networks (IJCNN)</em>, 2025.
  <strong>[<a href="https://arxiv.org/pdf/2309.12029.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/cyfml/OPSTL">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Towards Human-Understandable Visual Recognition for Nonexperts in Industrial Inspection: A Case Study for Car Manufacturing Lines.</strong><br>
  S. Sardari, F. Fernandes, J. M. Araya-Martinez, J. A. Zak, A. Roitberg.<br>
  <em>2025 IEEE 21st International Conference on Automation Science and Engineering (CASE)</em>, 2025, pp. 279‚Äì285. IEEE.
  <strong>[<a href="https://ieeexplore.ieee.org/abstract/document/11163755">IEEE link</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>AttentionLeak: What Does Human Attention Reveal About Information Visualisation?</strong><br>
  M. S√∂nnichsen, M. Elfares, Y. Wang, R. K√ºsters, A. Roitberg, A. Bulling.<br>
  <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, 2025, pp. 77‚Äì95. Cham: Springer Nature Switzerland.
  <strong>[<a href="https://publ.sec.uni-stuttgart.de/sonnichsenelfareswangkustersroitbergbulling-icdar-2025.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Building Foundation Models - Potentials, Challenges and Research Directions for Using LLM and LVM in AEC.</strong><br>
  J. Ploennigs, M. Berger, T. Wortmann, J. Kirchner, J. Beetz, A. Roitberg, K. Menzel, B. Ommer.<br>
  <em>Proceedings of the 2025 European Conference on Computing in Construction (EC¬≥)</em>, Porto, Portugal, 2025.
  <strong>[<a href="https://ec-3.org/publications/conferences/EC32025/papers/EC32025_268.pdf">pdf</a>]</strong>
</li>



<!-- <div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2024</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler.</strong><br>
  K. Peng, D. Wen, K. Yang, A. Luo, Y. Chen, J. Fu, M. S. Sarfraz, A. Roitberg, R. Stiefelhagen.<br>
  <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024.
  <strong>[<a href="https://arxiv.org/pdf/2409.17555">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/EBiL-HaDS">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Referring Atomic Video Action Recognition.</strong><br>
  K. Peng, J. Fu, K. Yang, D. Wen, Y. Chen, R. Liu, J. Zheng, J. Zhang, M. S. Sarfraz, R. Stiefelhagen, A. Roitberg.<br>
  <em>European Conference on Computer Vision (ECCV)</em>, Milan, Italy, 2024 .
  <strong>[<a href="https://arxiv.org/pdf/2407.01872">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/RAVAR">data+code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Skeleton-Based Human Action Recognition with Noisy Labels.</strong><br>
  Y. Xu, K. Peng, D. Wen, R. Liu, J. Zheng, Y. Chen, J. Zhang, A. Roitberg, K. Yang, R. Stiefelhagen.<br>
  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024.
  <strong>[<a href="https://arxiv.org/pdf/2403.09975">pdf</a>]</strong>
  <strong>[<a href="https://github.com/xuyizdby/NoiseEraSAR">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>SynthAct: Towards Generalizable Human Action Recognition based on Synthetic Data.</strong><br>
  D. Schneider, M. Keller, Z. Zhong, K. Peng, A. Roitberg, J. Beyerer, R. Stiefelhagen.<br>
  <em>IEEE International Conference onRobotics and Automation (ICRA)</em>, Yokohama, Japan, May 2024.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Chart4blind: An intelligent interface for chart accessibility conversion.</strong><br>
  O. Moured, M. Baumgarten-Egemole, K. M√ºller, A. Roitberg, T. Schwarz, R. Stiefelhagen.<br>
  <em>Proceedings of the 29th International Conference on Intelligent User Interfaces</em>, March 2024, pp. 504-514.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>TransKD: Transformer Knowledge Distillation for Efficient Semantic Segmentation.</strong><br>
  R. Liu, K. Yang, A. Roitberg, J. Zhang, K. Peng, H. Liu, Y. Wang, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2024.
  <strong>[<a href="https://arxiv.org/pdf/2202.13393">pdf</a>]</strong>
  <strong>[<a href="https://github.com/RuipingL/TransKD">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#29523a; background:#eef8f0;">BOOK CHAPTER</span>
  <strong>Bildbasierte Baufortschritts√ºberwachung.</strong><br>
  M. Koulakis, A. Albrecht, M. Wagner, A. Richter, F. Andres, A. Roitberg, J. Petereit, R. Stiefelhagen.<br>
  <em>K√ºnstliche Intelligenz im Bauwesen: Grundlagen und Anwendungsf√§lle</em>, May 10, 2024, pp. 205‚Äì219. Wiesbaden: Springer Fachmedien Wiesbaden. <strong>(book chapter)</strong>
  <strong>[<a href="https://doi.org/10.1007/978-3-658-42796-2_12">DOI</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Navigating Open Set Scenarios for Skeleton-based Action Recognition.</strong><br>
  K. Peng, C. Yin, J. Zheng, R. Liu, D. Schneider, J. Zhang, K. Yang, M. S. Sarfraz, R. Stiefelhagen, A. Roitberg.<br>
  <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, Vancouver, Canada, February 2024.
  <strong>[<a href="https://arxiv.org/pdf/2312.06330.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/OS-SAR" target="_blank">data+code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Elevating Skeleton-based Action Recognition with Efficient Multi-modality Self-supervision.</strong><br>
  Y. Wei, K. Peng, A. Roitberg, J. Zhang, J. Zheng, R. Liu, Y. Chen, K. Yang, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, Seoul, Korea, April 2024.
  <strong>[<a href="https://arxiv.org/pdf/2309.12009.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/desehuileng0o0/IKEM" target="_blank">code</a>]</strong>
</li>

<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #b7c9ff; border-radius:4px; font-size:0.75em; color:#1f3b7a; background:#eef3ff;">JOURNAL</span>
  <strong>Managing uncertainty in product and process design for the circular factory.</strong><br>
  M. Heizmann, J. Beyerer, S. Dietrich, L. Hoffmann, J.-P. Kaiser, G. Lanza, A. Roitberg, R. Stiefelhagen, N. Stricker, H. Wexel, F. Zanger.<br>
  <em>Automatisierungstechnik</em>, 72(9): 829‚Äì843, 2024.
  <strong>[<a href="https://doi.org/10.1515/auto-2024-0009">doi</a>]</strong>
</li>

<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #b7c9ff; border-radius:4px; font-size:0.75em; color:#1f3b7a; background:#eef3ff;">JOURNAL</span>
  <strong>Learning human actions from complex manipulation tasks and their transfer to robots in the circular factory.</strong><br>
  M. Zaremski, B. Handwerker, C. R. G. Dreher, F. Leven, D. Schneider, A. Roitberg, R. Stiefelhagen, G. Neumann, M. Heizmann, T. Asfour, B. Deml.<br>
  <em>Automatisierungstechnik</em>, 72(9): 844‚Äì860, 2024.
  <strong>[<a href="https://doi.org/10.1515/auto-2024-0008">doi</a>]</strong>
</li>

<!-- 2024 (CONF PROC) -->
<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #bfe6cf; border-radius:4px; font-size:0.75em; color:#1f5a3a; background:#effaf3;">CONF PROC</span>
  <strong>Towards Video-based Activated Muscle Group Estimation in the Wild.</strong><br>
  K. Peng, D. Schneider, A. Roitberg, K. Yang, J. Zhang, C. Deng, K. Zhang, M. Saquib Sarfraz, R. Stiefelhagen.<br>
  <em>ACM Multimedia (ACM MM)</em>, 2024, pp. 4495‚Äì4504.
  <strong>[<a href="https://arxiv.org/pdf/2303.00952.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/MuscleMap">code</a>]</strong>
</li>



<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2023</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments.</strong><br>
  C. Tanama, K. Peng, Z. Marinov, R. Stiefelhagen, A. Roitberg.<br>
  <em>IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2023.
  <strong>[<a href="https://arxiv.org/abs/2311.05970">pdf</a>]</strong>
  <strong>[<a href="https://github.com/calvintanama/qd-driver-activity-reco" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions.</strong><br>
  K. Peng, A. Roitberg, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Multimedia (TMM)</em>, 2023.
  <strong>[<a href="https://arxiv.org/pdf/2202.11423.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/Trans4SOAR" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Line Graphics Digitization: A Step Towards Full Automation.</strong><br>
  O. Moured, J. Zhang, A. Roitberg, T. Schwarz, R. Stiefelhagen.<br>
  <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, 2023. Cham: Springer Nature Switzerland.
  <strong>[<a href="https://arxiv.org/pdf/2307.02065.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/moured/Document-Graphics-Digitization.git" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation.</strong><br>
  H. Xiao, K. Peng, X. Huang, A. Roitberg, H. Li, Z. Wang, R. Stiefelhagen.<br>
  <em>IEEE Sensors Journal</em>, 2023.
  <strong>[<a href="https://arxiv.org/pdf/2308.12049">pdf</a>]</strong>
  <strong>[<a href="https://github.com/1015206533/privacy_supporting_fall_detection" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>On Transferability of Driver Observation Models from Simulated to Real Environments in Autonomous Cars.</strong><br>
  W. Morales-Alvarez*, N. Certad*, A. Roitberg*, R. Stiefelhagen, C. Olaverri-Monreal.<br>
  <em>IEEE Conference on Intelligent Transportation Systems (ITSC)</em>, 2023.
  <strong>[<a href="https://arxiv.org/pdf/2307.16543.pdf">pdf</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #ffd7b5; border-radius:4px; font-size:0.75em; color:#7a3a12; background:#fff4ea;">WORKSHOP PROC</span>
  <strong>FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation.</strong><br>
  H. Shi, Y. Li, K. Yang, J. Zhang, K. Peng, A. Roitberg, Y. Ye, H. Ni, K. Wang, R. Stiefelhagen.<br>
  <em>CVPR Workshops (OmniCV)</em>, 2023, pp. 6434‚Äì6444.
  <strong>[<a href="https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Shi_FishDreamer_Towards_Fisheye_Semantic_Completion_via_Unified_Image_Outpainting_and_CVPRW_2023_paper.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/MasterHow/FishDreamer">code</a>]</strong>
</li>



<!-- <div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2022</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Daily Living Activities.</strong><br>
  Z. Marinov*, A. Roitberg*, D. Schneider*, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2208.01910.pdf">pdf</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration.</strong><br>
  K. Peng, A. Roitberg, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2203.00927.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/TransDARC" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Multi-modal Depression Estimation based on Sub-attentional Fusion.</strong><br>
  P.-C. Wei, K. Peng, A. Roitberg, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>International Workshop on Assistive Computer Vision and Robotics (ACVR) with European Conference on Computer Vision (ECCV)</em>, 2022.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Is my Model Overconfident? Uncertainty-aware Driver Observation with Reliable and Interpretable Confidence Estimates.</strong><br>
  A. Roitberg, K. Peng, D. Schneider, M. Koulakis, K. Yang, M. Martinez, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/TransITS_CARING_ReliableDriverObservation.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Pose-based Contrastive Learning for Domain Agnostic Activity Representations.</strong><br>
  D. Schneider, S. Sarfraz, A. Roitberg, R. Stiefelhagen.<br>
  <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Should I take a walk? Estimating Energy Expenditure from Video Data.</strong><br>
  K. Peng*, A. Roitberg*, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>CVPR Workshop on Computer Vision for Physiological Measurement</em>, June 2022.
  <strong>[<a href="https://arxiv.org/pdf/2202.00712" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/Vid2Burn" target="_blank">code</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense Top-View Understanding.</strong><br>
  K. Peng, J. Fei, K. Yang, A. Roitberg, J. Zhang, F. Bieder, P. Heidenreich, C. Stiller, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2107.00346.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/MASS" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Transfer beyond the Field of View: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation.</strong><br>
  J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2110.11062.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/chma1024/DensePASS" target="_blank">data</a>]</strong>
</li>



<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2021</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Affect-DML: Context-Aware One-Shot Recognition of Human Affect using Deep Metric Learning.</strong><br>
  K. Peng, A. Roitberg, D. Schneider, M. Koulakis, K. Yang, R. Stiefelhagen.<br>
  <em>International Conference on Automatic Face and Gesture Recognition</em>, December 2021.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games.</strong><br>
  A. Roitberg*, D. Schneider*, A. Djamal, C. Seibold, S. Rei√ü, R. Stiefelhagen.<br>
  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Prague, Czech Republic, September 2021.
  <strong>[<a href="https://arxiv.org/pdf/2107.05617.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/aroitberg/sims4action" target="_blank">website</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>From Driver Talk To Future Action: Vehicle Maneuver Prediction by Learning from Driving Exam Dialogs.</strong><br>
  A. Roitberg, S. Rei√ü, R. Stiefelhagen.<br>
  <em>IEEE Intelligent Vehicles Symposium</em>, Nagoya, Japan (virtual), July 2021.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Roitberg_IV2021_DriverTalk.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>DensePASS: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation with Attention-Augmented Context Exchange.</strong><br>
  C. Ma, J. Zhang, K. Yang, A. Roitberg, K. Peng, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Intelligent Transportation Systems (ITSC)</em>, 2021.
  <strong>[<a href="https://arxiv.org/pdf/2108.06383.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/chma1024/DensePASS" target="_blank">website</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#7a2a2a; background:#fdecec;">THESIS</span>
  <strong>Uncertainty-aware Models for Deep Learning-based Human Activity Recognition and Applications in Intelligent Vehicles.</strong><br>
  A. Roitberg.<br>
  <em>PhD Dissertation</em>, April 2021.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Dissertation_Roitberg_UncertaintyAwareDriverActivityRecognition.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Uncertainty-sensitive Activity Recognition: a Reliability Benchmark and the CARING Models.</strong><br>
  A. Roitberg, M. Haurilet, M. Martinez, R. Stiefelhagen.<br>
  <em>International Conference on Pattern Recognition (ICPR)</em>, January 2021, oral (&lt;10% acceptance rate for orals).
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/UncertaintyCARINGActionRecognition_ICPR2021.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Multi-Task Learning for Calorie Prediction on a Novel Large-Scale Recipe Dataset Enriched with Nutritional Information.</strong><br>
  R. Ruede, V. Heusser, L. Frank, A. Roitberg, M. Haurilet, R. Stiefelhagen.<br>
  <em>International Conference on Pattern Recognition (ICPR)</em>, January 2021.
  <strong>[<a href="https://arxiv.org/pdf/2011.01082.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://www.newscientist.com/article/2260415-computer-vision-can-estimate-calorie-content-of-food-at-a-glance/" target="_blank">press article</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Order Matters: On Parameter-Efficient Image-to-Video Probing for Recognizing Nearly Symmetric Actions.</strong><br>
  T. Thiyakesan Ponbagavathi, A. Roitberg.<br>
  <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2026 (accepted).
  <strong>[<a href="https://arxiv.org/pdf/2503.24298.pdf">pdf</a>]</strong>
  <span style="color:#666;"><strong>[code coming soon]</strong></span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Mitigating Label Noise using Prompt-Based Hyperbolic Meta-Learning in Open-Set Domain Generalization.</strong><br>
  K. Peng, D. Wen, M. Saquib Sarfraz, Y. Chen, J. Zheng, D. Schneider, K. Yang, J. Wu, A. Roitberg, R. Stiefelhagen.<br>
  <em>International Journal of Computer Vision (IJCV)</em>, 2026.
  <strong>[<a href="https://arxiv.org/pdf/2412.18342.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/HyProMeta">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>When Surgery Meets the Unknown: Uncertainty-Aware Open-Set Recognition for Surgery Phase Classification.</strong><br>
  S. Geyer, V. Kalogeiton, A. Roitberg.<br>
  <em>International Conference on Computer Vision Theory and Applications (VISAPP)</em>, 2026 (accepted).
  <span style="color:#666;"><strong>[pdf &amp; code coming soon]</strong></span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>PDNet: Exploring Pruning and Distillation for Real-Time Worker Activity Recognition in Industrial Assembly.</strong><br>
  M. Baba, T. Thiyakesan Ponbagavathi, A. Roitberg.<br>
  <em>Conference paper</em>, 2026 (accepted). <span style="color:#666;">(based on a master‚Äôs thesis)</span>
  <span style="color:#666;"><strong>[pdf &amp; code coming soon]</strong></span>
</li>



<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2025</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>T-MASK: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring.</strong><br>
  T. Thiyakesan Ponbagavathi, K. Peng, A. Roitberg.<br>
  <em>IEEE Intelligent Transportation Systems Conference (ITSC)</em>, Naples, Italy, 2025.
  <strong>üèÜ Best Student Paper Award Finalist (Top 5)</strong>.
  <strong>[<a href="https://arxiv.org/pdf/2508.16207.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/th-nesh/T-MASK">code coming soon</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Deep Learning for Metabolic Rate Estimation from Biosignals: A Comparative Study of Architectures and Signal Selection.</strong><br>
  S. Babakhani, C. David Remy, A. Roitberg.<br>
  <em>BMVC 2025 Workshop on Multisensory Intelligence for Human Perception</em>, 2025.
  <strong>[<a href="https://bmva-archive.org.uk/bmvc/2025/assets/workshops/MPI/Paper_1/paper.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/Sarvibabakhani/deeplearning-biosignals-ee">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Exploring Self-supervised Skeleton-based Action Recognition in Occluded Environments.</strong><br>
  Y. Chen, K. Peng, A. Roitberg, D. Schneider, J. Zhang, J. Zheng, Y. Chen, R. Liu, K. Yang, R. Stiefelhagen.<br>
  <em>International Joint Conference on Neural Networks (IJCNN)</em>, 2025.
  <strong>[<a href="https://arxiv.org/pdf/2309.12029.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/cyfml/OPSTL">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Towards Human-Understandable Visual Recognition for Nonexperts in Industrial Inspection: A Case Study for Car Manufacturing Lines.</strong><br>
  S. Sardari, F. Fernandes, J. M. Araya-Martinez, J. A. Zak, A. Roitberg.<br>
  <em>2025 IEEE 21st International Conference on Automation Science and Engineering (CASE)</em>, 2025, pp. 279‚Äì285. IEEE.
  <strong>[<a href="https://ieeexplore.ieee.org/abstract/document/11163755">IEEE link</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>AttentionLeak: What Does Human Attention Reveal About Information Visualisation?</strong><br>
  M. S√∂nnichsen, M. Elfares, Y. Wang, R. K√ºsters, A. Roitberg, A. Bulling.<br>
  <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, 2025, pp. 77‚Äì95. Cham: Springer Nature Switzerland.
  <strong>[<a href="https://publ.sec.uni-stuttgart.de/sonnichsenelfareswangkustersroitbergbulling-icdar-2025.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Building Foundation Models - Potentials, Challenges and Research Directions for Using LLM and LVM in AEC.</strong><br>
  J. Ploennigs, M. Berger, T. Wortmann, J. Kirchner, J. Beetz, A. Roitberg, K. Menzel, B. Ommer.<br>
  <em>Proceedings of the 2025 European Conference on Computing in Construction (EC¬≥)</em>, Porto, Portugal, 2025.
  <strong>[<a href="https://ec-3.org/publications/conferences/EC32025/papers/EC32025_268.pdf">pdf</a>]</strong>
</li>



<!-- <div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2024</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler.</strong><br>
  K. Peng, D. Wen, K. Yang, A. Luo, Y. Chen, J. Fu, M. S. Sarfraz, A. Roitberg, R. Stiefelhagen.<br>
  <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024.
  <strong>[<a href="https://arxiv.org/pdf/2409.17555">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/EBiL-HaDS">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Referring Atomic Video Action Recognition.</strong><br>
  K. Peng, J. Fu, K. Yang, D. Wen, Y. Chen, R. Liu, J. Zheng, J. Zhang, M. S. Sarfraz, R. Stiefelhagen, A. Roitberg.<br>
  <em>European Conference on Computer Vision (ECCV)</em>, Milan, Italy, 2024.
  <strong>[<a href="https://arxiv.org/pdf/2407.01872">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/RAVAR">data+code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Skeleton-Based Human Action Recognition with Noisy Labels.</strong><br>
  Y. Xu, K. Peng, D. Wen, R. Liu, J. Zheng, Y. Chen, J. Zhang, A. Roitberg, K. Yang, R. Stiefelhagen.<br>
  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024.
  <strong>[<a href="https://arxiv.org/pdf/2403.09975">pdf</a>]</strong>
  <strong>[<a href="https://github.com/xuyizdby/NoiseEraSAR">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>SynthAct: Towards Generalizable Human Action Recognition based on Synthetic Data.</strong><br>
  D. Schneider, M. Keller, Z. Zhong, K. Peng, A. Roitberg, J. Beyerer, R. Stiefelhagen.<br>
  <em>IEEE International Conference onRobotics and Automation (ICRA)</em>, Yokohama, Japan, May 2024.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Chart4blind: An intelligent interface for chart accessibility conversion.</strong><br>
  O. Moured, M. Baumgarten-Egemole, K. M√ºller, A. Roitberg, T. Schwarz, R. Stiefelhagen.<br>
  <em>Proceedings of the 29th International Conference on Intelligent User Interfaces</em>, March 2024, pp. 504-514.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>TransKD: Transformer Knowledge Distillation for Efficient Semantic Segmentation.</strong><br>
  R. Liu, K. Yang, A. Roitberg, J. Zhang, K. Peng, H. Liu, Y. Wang, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2024.
  <strong>[<a href="https://arxiv.org/pdf/2202.13393">pdf</a>]</strong>
  <strong>[<a href="https://github.com/RuipingL/TransKD">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#29523a; background:#eef8f0;">BOOK CHAPTER</span>
  <strong>Bildbasierte Baufortschritts√ºberwachung.</strong><br>
  M. Koulakis, A. Albrecht, M. Wagner, A. Richter, F. Andres, A. Roitberg, J. Petereit, R. Stiefelhagen.<br>
  <em>K√ºnstliche Intelligenz im Bauwesen: Grundlagen und Anwendungsf√§lle</em>, May 10, 2024, pp. 205‚Äì219. Wiesbaden: Springer Fachmedien Wiesbaden. <strong>(book chapter)</strong>
  <strong>[<a href="https://doi.org/10.1007/978-3-658-42796-2_12">DOI</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Navigating Open Set Scenarios for Skeleton-based Action Recognition.</strong><br>
  K. Peng, C. Yin, J. Zheng, R. Liu, D. Schneider, J. Zhang, K. Yang, M. S. Sarfraz, R. Stiefelhagen, A. Roitberg.<br>
  <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, Vancouver, Canada, February 2024.
  <strong>[<a href="https://arxiv.org/pdf/2312.06330.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/OS-SAR" target="_blank">data+code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Elevating Skeleton-based Action Recognition with Efficient Multi-modality Self-supervision.</strong><br>
  Y. Wei, K. Peng, A. Roitberg, J. Zhang, J. Zheng, R. Liu, Y. Chen, K. Yang, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, Seoul, Korea, April 2024.
  <strong>[<a href="https://arxiv.org/pdf/2309.12009.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/desehuileng0o0/IKEM" target="_blank">code</a>]</strong>
</li>

<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #b7c9ff; border-radius:4px; font-size:0.75em; color:#1f3b7a; background:#eef3ff;">JOURNAL</span>
  <strong>Managing uncertainty in product and process design for the circular factory.</strong><br>
  M. Heizmann, J. Beyerer, S. Dietrich, L. Hoffmann, J.-P. Kaiser, G. Lanza, A. Roitberg, R. Stiefelhagen, N. Stricker, H. Wexel, F. Zanger.<br>
  <em>Automatisierungstechnik</em>, 72(9): 829‚Äì843, 2024.
  <strong>[<a href="https://doi.org/10.1515/auto-2024-0009">doi</a>]</strong>
</li>

<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #b7c9ff; border-radius:4px; font-size:0.75em; color:#1f3b7a; background:#eef3ff;">JOURNAL</span>
  <strong>Learning human actions from complex manipulation tasks and their transfer to robots in the circular factory.</strong><br>
  M. Zaremski, B. Handwerker, C. R. G. Dreher, F. Leven, D. Schneider, A. Roitberg, R. Stiefelhagen, G. Neumann, M. Heizmann, T. Asfour, B. Deml.<br>
  <em>Automatisierungstechnik</em>, 72(9): 844‚Äì860, 2024.
  <strong>[<a href="https://doi.org/10.1515/auto-2024-0008">doi</a>]</strong>
</li>

<!-- 2024 (CONF PROC) -->
<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #bfe6cf; border-radius:4px; font-size:0.75em; color:#1f5a3a; background:#effaf3;">CONF PROC</span>
  <strong>Towards Video-based Activated Muscle Group Estimation in the Wild.</strong><br>
  K. Peng, D. Schneider, A. Roitberg, K. Yang, J. Zhang, C. Deng, K. Zhang, M. Saquib Sarfraz, R. Stiefelhagen.<br>
  <em>ACM Multimedia (ACM MM)</em>, 2024, pp. 4495‚Äì4504.
  <strong>[<a href="https://arxiv.org/pdf/2303.00952.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/MuscleMap">code</a>]</strong>
</li>



<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2023</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments.</strong><br>
  C. Tanama, K. Peng, Z. Marinov, R. Stiefelhagen, A. Roitberg.<br>
  <em>IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2023.
  <strong>[<a href="https://arxiv.org/abs/2311.05970">pdf</a>]</strong>
  <strong>[<a href="https://github.com/calvintanama/qd-driver-activity-reco" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions.</strong><br>
  K. Peng, A. Roitberg, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Multimedia (TMM)</em>, 2023.
  <strong>[<a href="https://arxiv.org/pdf/2202.11423.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/Trans4SOAR" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Line Graphics Digitization: A Step Towards Full Automation.</strong><br>
  O. Moured, J. Zhang, A. Roitberg, T. Schwarz, R. Stiefelhagen.<br>
  <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, 2023. Cham: Springer Nature Switzerland.
  <strong>[<a href="https://arxiv.org/pdf/2307.02065.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/moured/Document-Graphics-Digitization.git" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation.</strong><br>
  H. Xiao, K. Peng, X. Huang, A. Roitberg, H. Li, Z. Wang, R. Stiefelhagen.<br>
  <em>IEEE Sensors Journal</em>, 2023.
  <strong>[<a href="https://arxiv.org/pdf/2308.12049">pdf</a>]</strong>
  <strong>[<a href="https://github.com/1015206533/privacy_supporting_fall_detection" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>On Transferability of Driver Observation Models from Simulated to Real Environments in Autonomous Cars.</strong><br>
  W. Morales-Alvarez*, N. Certad*, A. Roitberg*, R. Stiefelhagen, C. Olaverri-Monreal.<br>
  <em>IEEE Conference on Intelligent Transportation Systems (ITSC)</em>, 2023.
  <strong>[<a href="https://arxiv.org/pdf/2307.16543.pdf">pdf</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li style="list-style:none; margin: 6px 0;">
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #ffd7b5; border-radius:4px; font-size:0.75em; color:#7a3a12; background:#fff4ea;">WORKSHOP PROC</span>
  <strong>FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation.</strong><br>
  H. Shi, Y. Li, K. Yang, J. Zhang, K. Peng, A. Roitberg, Y. Ye, H. Ni, K. Wang, R. Stiefelhagen.<br>
  <em>CVPR Workshops (OmniCV)</em>, 2023, pp. 6434‚Äì6444.
  <strong>[<a href="https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Shi_FishDreamer_Towards_Fisheye_Semantic_Completion_via_Unified_Image_Outpainting_and_CVPRW_2023_paper.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/MasterHow/FishDreamer">code</a>]</strong>
</li>



<!-- <div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2022</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Daily Living Activities.</strong><br>
  Z. Marinov*, A. Roitberg*, D. Schneider*, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2208.01910.pdf">pdf</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration.</strong><br>
  K. Peng, A. Roitberg, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2203.00927.pdf">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/TransDARC" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Multi-modal Depression Estimation based on Sub-attentional Fusion.</strong><br>
  P.-C. Wei, K. Peng, A. Roitberg, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>International Workshop on Assistive Computer Vision and Robotics (ACVR) with European Conference on Computer Vision (ECCV)</em>, 2022.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Is my Model Overconfident? Uncertainty-aware Driver Observation with Reliable and Interpretable Confidence Estimates.</strong><br>
  A. Roitberg, K. Peng, D. Schneider, M. Koulakis, K. Yang, M. Martinez, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/TransITS_CARING_ReliableDriverObservation.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Pose-based Contrastive Learning for Domain Agnostic Activity Representations.</strong><br>
  D. Schneider, S. Sarfraz, A. Roitberg, R. Stiefelhagen.<br>
  <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Should I take a walk? Estimating Energy Expenditure from Video Data.</strong><br>
  K. Peng*, A. Roitberg*, K. Yang, J. Zhang, R. Stiefelhagen.<br>
  <em>CVPR Workshop on Computer Vision for Physiological Measurement</em>, June 2022.
  <strong>[<a href="https://arxiv.org/pdf/2202.00712" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/Vid2Burn" target="_blank">code</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense Top-View Understanding.</strong><br>
  K. Peng, J. Fei, K. Yang, A. Roitberg, J. Zhang, F. Bieder, P. Heidenreich, C. Stiller, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2107.00346.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/KPeng9510/MASS" target="_blank">code</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Transfer beyond the Field of View: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation.</strong><br>
  J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, R. Stiefelhagen.<br>
  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022.
  <strong>[<a href="https://arxiv.org/pdf/2110.11062.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/chma1024/DensePASS" target="_blank">data</a>]</strong>
</li>



<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2021</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Affect-DML: Context-Aware One-Shot Recognition of Human Affect using Deep Metric Learning.</strong><br>
  K. Peng, A. Roitberg, D. Schneider, M. Koulakis, K. Yang, R. Stiefelhagen.<br>
  <em>International Conference on Automatic Face and Gesture Recognition</em>, December 2021.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games.</strong><br>
  A. Roitberg*, D. Schneider*, A. Djamal, C. Seibold, S. Rei√ü, R. Stiefelhagen.<br>
  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Prague, Czech Republic, September 2021.
  <strong>[<a href="https://arxiv.org/pdf/2107.05617.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/aroitberg/sims4action" target="_blank">website</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>From Driver Talk To Future Action: Vehicle Maneuver Prediction by Learning from Driving Exam Dialogs.</strong><br>
  A. Roitberg, S. Rei√ü, R. Stiefelhagen.<br>
  <em>IEEE Intelligent Vehicles Symposium</em>, Nagoya, Japan (virtual), July 2021.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Roitberg_IV2021_DriverTalk.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>DensePASS: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation with Attention-Augmented Context Exchange.</strong><br>
  C. Ma, J. Zhang, K. Yang, A. Roitberg, K. Peng, R. Stiefelhagen.<br>
  <em>IEEE International Conference on Intelligent Transportation Systems (ITSC)</em>, 2021.
  <strong>[<a href="https://arxiv.org/pdf/2108.06383.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://github.com/chma1024/DensePASS" target="_blank">website</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#7a2a2a; background:#fdecec;">THESIS</span>
  <strong>Uncertainty-aware Models for Deep Learning-based Human Activity Recognition and Applications in Intelligent Vehicles.</strong><br>
  A. Roitberg.<br>
  <em>PhD Dissertation</em>, April 2021.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Dissertation_Roitberg_UncertaintyAwareDriverActivityRecognition.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Uncertainty-sensitive Activity Recognition: a Reliability Benchmark and the CARING Models.</strong><br>
  A. Roitberg, M. Haurilet, M. Martinez, R. Stiefelhagen.<br>
  <em>International Conference on Pattern Recognition (ICPR)</em>, January 2021, oral (&lt;10% acceptance rate for orals).
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/UncertaintyCARINGActionRecognition_ICPR2021.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Multi-Task Learning for Calorie Prediction on a Novel Large-Scale Recipe Dataset Enriched with Nutritional Information.</strong><br>
  R. Ruede, V. Heusser, L. Frank, A. Roitberg, M. Haurilet, R. Stiefelhagen.<br>
  <em>International Conference on Pattern Recognition (ICPR)</em>, January 2021.
  <strong>[<a href="https://arxiv.org/pdf/2011.01082.pdf" target="_blank">pdf</a>]</strong>
  <strong>[<a href="https://www.newscientist.com/article/2260415-computer-vision-can-estimate-calorie-content-of-food-at-a-glance/" target="_blank">press article</a>]</strong>
</li>


<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2020</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#29523a; background:#eef8f0;">BOOK CHAPTER</span>
  <strong>Personalisation and Control Transition Between Automation and Driver in Highly Automated Cars.</strong><br>
  M. Flad, P. Karg, P, A. Roitberg, M. Martin, M. Mazewitsch, C. Lange, E. Kenar, L. Ahrens, B. Flecken, L. Kalb, B. Karakaya, J. Ludwig, A. Pruksch, R. Stiefelhagen, S. Hohmann.<br>
  <em>Smart Automotive Mobility. Human‚ÄìComputer Interaction Series. Springer</em>, 2020.
  <strong>[<a href="https://doi.org/10.1007/978-3-030-45131-8_1" target="_blank">doi</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Open Set Driver Activity Recognition.</strong><br>
  A. Roitberg, C. Ma, M. Haurilet, R. Stiefelhagen.<br>
  <em>Intelligent Vehicles Symposium (IV)</em>, IEEE, October 2020,
  <strong>üèÜ Best Student Paper 2nd Place Award</strong>.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/OpenSetDriverActivityRecognition_IV2020.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Deep Classification-driven Domain Adaptation for Cross-Modal Driver Behavior Recognition.</strong><br>
  S. Rei√ü*, A. Roitberg*, M. Haurilet, R. Stiefelhagen.<br>
  <em>Intelligent Vehicles Symposium (IV)</em>, IEEE, October 2020.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/CrossDomainDriverActivityRecognition_IV2020.pdf">pdf</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>CNN-based Driver Activity Recognition: Shedding Light on Deep Spatiotemporal Representations.</strong><br>
  A. Roitberg, M. Haurilet, S. Rei√ü, R. Stiefelhagen.<br>
  <em>International Conference on Intelligent Transportation Systems (ITSC)</em>, IEEE, September 2020.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/InterpretableCNNsDriverObservation_ITSC2020.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Activity-aware Attributes for Zero-Shot Driver Behavior Recognition.</strong><br>
  S. Rei√ü*, A. Roitberg*, M. Haurilet, R. Stiefelhagen.<br>
  <em>CVPR Workshop on Visual Learning with Limited Labels (VL-LL)</em>, IEEE, June 2020.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/AttributesZeroShotDriverActivity_CVPRW2020.pdf">pdf</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>


<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2019</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Drive&amp;Act: A Multi-modal Dataset for Fine-grained Driver Behavior Recognition in Autonomous Vehicles.</strong><br>
  M. Martin*, A. Roitberg*, M. Haurilet, M. Horne, S. Rei√ü, M. Voit, R. Stiefelhagen.<br>
  <em>International Conference on Computer Vision (ICCV)</em>, IEEE, Seoul, South Korea, Oct. 2019.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/DriveAndAct_ICCV2019.pdf">pdf</a>]</strong>
  <strong>[<a href="http://cvhci.anthropomatik.kit.edu/~aroitberg/publications/DriveAndAct_ICCV2019.bib">bib</a>]</strong>
  <span style="color:#666;">(25.0% acceptance rate, * denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>WiSe - Slide Segmentation in the Wild.</strong><br>
  M. Haurilet, A. Roitberg, M. Martinez, R. Stiefelhagen.<br>
  <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, IEEE, Sydney, Australia, Sep. 2019.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~mhaurile/papers/2019_ICDAR_WiSe.pdf">pdf</a>]</strong>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Learning Fine-Grained Image Representations for Mathematical Expression Recognition.</strong><br>
  S. Bender*, M. Haurilet*, A. Roitberg, R. Stiefelhagen.<br>
  <em>ICDAR Workshop on Graphics Recognition</em>, Sydney, Australia, Sep. 2019.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~mhaurile/papers/2019_GREC_FGFE.pdf">pdf</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>End-to-end Prediction of Driver Intention using 3D Convolutional Neural Networks.</strong><br>
  P. Gebert*, A. Roitberg*, M. Haurilet, R. Stiefelhagen.<br>
  <em>Intelligent Vehicles Symposium (IV)</em>, IEEE, Paris, France, June 2019.
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>It‚Äôs not about the Journey; It‚Äôs about the Destination: Following Soft Paths under Question-Guidance for Visual Reasoning.</strong><br>
  M. Haurilet, A. Roitberg, R. Stiefelhagen.<br>
  <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, IEEE, Long Beach, USA, June 2019.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~mhaurile/papers/CVPR2019_SoftPaths.pdf">pdf</a>]</strong>
  <span style="color:#666;">(25.2% acceptance rate)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Analysis of Deep Fusion Strategies for Multi-modal Gesture Recognition.</strong><br>
  A. Roitberg*, T. Pollert*, M. Haurilet, M. Martin, R. Stiefelhagen.<br>
  <em>CVPR Workshop on Analysis and Modeling of Faces and Gestures (AMFG)</em>, IEEE, Long Beach, USA, June 2019.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/DeepFusionGestureRecognition_AMFG_CVPRW2019.pdf">pdf</a>]</strong>
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/roitbergCVPRW2019DeepFusion.bib">bib</a>]</strong>
  <span style="color:#666;">(* denotes equal contribution)</span>
</li>

<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2018</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Informed Democracy: Voting-based Novelty Detection for Action Recognition.</strong><br>
  A. Roitberg, Z. Al-Halah, R. Stiefelhagen.<br>
  <em>British Machine Vision Conference (BMVC)</em>, 2018.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Roitberg2018a_BMVC_NoveltyDetection.pdf">pdf</a>]</strong>
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Poster_Roitberg2018a_BMVC18_NoveltyDetection.pdf">poster</a>]</strong>
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/roitbergBMVC2018novelty.bib">bib</a>]</strong>
  <span style="color:#666;">(29.5% acceptance rate)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Towards a Fair Evaluation of Zero-Shot Action Recognition using External Data.</strong><br>
  A. Roitberg, M. Martinez, M. Haurilet, R. Stiefelhagen.<br>
  <em>ECCV Workshop on Shortcomings in Vision and Language (SiVL)</em>, Springer, Munich, Germany, 2018.
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Roitberg2018b_SiVL_ECCVWorkshop_ZeroShotAction.pdf">pdf</a>]</strong>
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/Poster_Roitberg2018b_SiVL_ECCVWorkshop_ZeroShotAction.pdf">poster</a>]</strong>
  <strong>[<a href="https://cvhci.anthropomatik.kit.edu/~aroitberg/publications/roitbergSiVL2018ZSAction.bib">bib</a>]</strong>
  <span style="color:#666;">(spotlight presentation)</span>
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Estimating Mental Load in Passive and Active Tasks from Pupil and Gaze Changes using Bayesian Surprise.</strong><br>
  E. Wolf, M. Martinez, A. Roitberg, R. Stiefelhagen, B. Deml.<br>
  <em>ACM ICMI Modeling Cognitive Processes from Multimodal Data Workshop (ICMI-MCPMD)</em>, Boulder, Colorado, USA, 2018.
</li>


<!--<div style="text-align: center; font-size: 2em; font-weight: bold; margin-top: 20px; margin-bottom: 20px;">2017 and prior</div>-->

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#5a4a1f; background:#fbf3e9;">WORKSHOP PROC</span>
  <strong>Using Technology Developed for Autonomous Cars to Help Navigate Blind People.</strong><br>
  M. Martinez, A. Roitberg, D. Koester, B. Schauerte, R. Stiefelhagen.<br>
  <em>ICCV Workshop on Assistive Computer Vision and Robotics (ACVR)</em>, 2017.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#4a2a6a; background:#f3ecfb;">JOURNAL</span>
  <strong>Connecting Artificial Brains to Robots in a Comprehensive Simulation Framework: The Neurorobotics Platform.</strong><br>
  E. Falotico, L. Vannucci, A. Ambrosano, U. Albanese, S. Ulbrich, J. C. Vasquez Tieck, G. Hinkel, J. Kaiser, I. Peric, O. Denninger, N. Cauli, M. Kirtay, A. Roennau, G. Klinker, A. Von Arnim, L. Guyot, D. Peppicelli, P. Mart√≠nez-Ca√±ada, E. Ros, P. Maier, S. Weber, M. Huber, D. Plecher, F. R√∂hrbein, S. Deser, A. Roitberg, P. van der Smagt, R. Dillman, P. Levi, C. Laschi, A. C. Knoll, M.-O. Gewaltig.<br>
  <em>Frontiers in Neurorobotics</em>, 2017.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Improved skeleton estimation by means of depth data fusion from multiple depth cameras.</strong><br>
  M. Carraro, M. Munaro, A. Roitberg, E. Menegatti.<br>
  <em>International Conference on Intelligent Autonomous Systems</em>, Springer, 2016.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Multimodal human activity recognition for industrial manufacturing processes in robotic workcells.</strong><br>
  A. Roitberg, N. Somani, A. Perzylo, M. Rickert, A. Knoll.<br>
  <em>ACM International Conference on Multimodal Interaction (ICMI)</em>, Seattle, USA, 2015.
</li>

<li>
  <span style="display:inline-block; padding:2px 6px; margin-right:6px; border:1px solid #cfd8d3; border-radius:4px; font-size:0.75em; color:#1f4b5a; background:#e9f6fb;">CONF PROC</span>
  <strong>Human Activity Recognition in the Context of Industrial Human-Robot Interaction.</strong><br>
  A. Roitberg, A. Perzylo, N. Somani, M. Giuliani, M. Rickert, A. Knoll.<br>
  <em>Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC 2014)</em>, IEEE, Siem Reap, Cambodia, 2014.
</li>



</ul>
			</div>
		</div> <!-- /section -->

	</div>	<!-- /container -->

</main>






<!-- JavaScript libs are placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="assets/js/template.js"></script>
<script>
    $(document).ready(function(){
        $('.navbar-toggle').collapse();
    });
</script>
</body>
</html>
